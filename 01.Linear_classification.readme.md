
# README: 선형 데이터 분석을 위한 핵심 ML 알고리즘

선형적인 경향을 보이는 데이터셋(특성 간 또는 특성과 목표 변수 간의 관계가 직선 형태인 경우)을 효율적으로 분석하고 모델링하기 위한 주요 알고리즘들을 정리했습니다.

## 1\. 선형 회귀 (Linear Regression) 및 로지스틱 회귀 (Logistic Regression)

선형 관계 데이터 분석의 기본이 되는 모델이며, 모델 해석(Coefficients)이 매우 용이합니다.

### 핵심 개념

  * **선형 회귀**: 연속형 목표 변수(예: 가격, 매출) 예측.
  * **로지스틱 회귀**: **확률 기반** 분류 모델 (이진 및 다중 분류). 출력은 $\mathbf{0}$과 $\mathbf{1}$ 사이의 확률이며, 선형 모델 중 가장 널리 쓰이는 분류기입니다.

### R 코드 (로지스틱 회귀 - $\texttt{glmnet}$)

```r
library(caret)
# L1/L2 규제가 포함된 로지스틱 회귀 (Elastic Net)
model_logistic <- train(
  x = X_train_pca,
  y = y_train,
  method = "glmnet", # L1(Lasso) + L2(Ridge) 규제 사용
  trControl = fit_control,
  # alpha = 1 (Lasso), alpha = 0 (Ridge)
  tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-3, 1, length.out = 5)) 
)
cat(sprintf("Best lambda: %.4f", model_logistic$bestTune$lambda))
```

### Python 코드 (로지스틱 회귀 - $\texttt{sklearn}$)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.01, 0.1, 1, 10, 100]} # C: 규제 역수 (C↑ -> 규제↓)
model_logistic = LogisticRegression(
    multi_class='multinomial', # 다중 분류 설정
    solver='lbfgs', 
    max_iter=1000
)

grid_search = GridSearchCV(model_logistic, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_pca, y_train)

print(f"Best C: {grid_search.best_params_['C']}")
```

-----

## 2\. 선형 SVM (Linear Support Vector Machine)

선형 경계를 찾는 데 있어 가장 강력한 모델 중 하나입니다.

### 📌 핵심 개념

  * **최대 마진 경계**: 클래스 간 \*\*마진(Margin)\*\*을 최대화하는 초평면(Hyperplane)을 찾아 일반화 성능을 높입니다.
  * **$C$ 파라미터**: 규제 강도를 조절합니다. $\mathbf{C}$ 값이 작을수록 규제가 강해져 오분류에 더 관대해집니다.

### 💻 R 코드 (Linear SVM - $\texttt{caret}$)

```r
library(caret)

grid_linear <- expand.grid(C = c(0.01, 0.1, 1, 10))

model_linear_svm <- train(
  x = X_train_pca,
  y = y_train,
  method = "svmLinear", # 선형 커널 명시
  trControl = fit_control,
  tuneGrid = grid_linear
)

cat(sprintf("Best Linear SVM C: %.2f", model_linear_svm$bestTune$C))
```

### Python 코드 (Linear SVM - $\texttt{sklearn}$)

```python
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.01, 0.1, 1, 10]}
# LinearSVC는 대규모 선형 데이터셋에 최적화되어 있습니다.
model_linear_svc = LinearSVC(dual=False, max_iter=5000) 

grid_search = GridSearchCV(model_linear_svc, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_pca, y_train)

print(f"Best Linear SVC C: {grid_search.best_params_['C']}")
```

-----

## 3\. 규제된 선형 모델 (Regularized Linear Models)

고차원 데이터($P \gg N$)에서 과적합을 방지하고 불필요한 특성을 제거하는 데 필수적입니다.

### 핵심 개념

  * **Ridge (L2 규제)**: 모든 계수를 0에 가깝게 줄여 과적합을 방지합니다.
  * **Lasso (L1 규제)**: 중요하지 않은 특성의 계수를 **완전히 0**으로 만들어 **특성 선택** 효과를 냅니다.
  * **$\mathbf{\lambda}$ (Lambda)**: 규제 강도 파라미터. $\mathbf{\lambda}$가 클수록 규제가 강해집니다.

### R 코드 (Elastic Net - $\texttt{glmnet}$)

```r
# Elastic Net (Ridge와 Lasso 혼합)
grid_elastic <- expand.grid(alpha = c(0, 0.5, 1), # 0: Ridge, 1: Lasso
                            lambda = 10^seq(-3, 1, length.out = 5))

model_elastic_net <- train(
  x = X_train_pca,
  y = y_train,
  method = "glmnet",
  trControl = fit_control,
  tuneGrid = grid_elastic
)

cat(sprintf("Best alpha: %.2f, Best lambda: %.4f", 
            model_elastic_net$bestTune$alpha, model_elastic_net$bestTune$lambda))
```

### Python 코드 (Elastic Net - $\texttt{sklearn}$)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

param_grid = {
    'l1_ratio': [0.1, 0.5, 0.9], # Lasso 비율 (alpha에 해당)
    'C': [0.1, 1, 10]
}
# solver='saga'는 Elastic Net 규제를 지원합니다.
model_elastic_logistic = LogisticRegression(
    penalty='elasticnet', 
    solver='saga', 
    max_iter=5000, 
    multi_class='multinomial'
)

grid_search = GridSearchCV(model_elastic_logistic, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_pca, y_train)

print(f"Best l1_ratio: {grid_search.best_params_['l1_ratio']}, Best C: {grid_search.best_params_['C']}")
```
